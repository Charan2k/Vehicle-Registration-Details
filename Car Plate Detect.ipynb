{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07e5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a5301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49190aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten, MaxPool2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178f8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plate(img): # the function detects and perfors blurring on the number plate.\n",
    "    plate_img = img.copy()\n",
    "    \n",
    "    #Loads the data required for detecting the license plates from cascade classifier.\n",
    "    plate_cascade = cv2.CascadeClassifier('./indian_plate.xml')\n",
    "\n",
    "    # detects numberplates and returns the coordinates and dimensions of detected license plate's contours.\n",
    "    plate_rect = plate_cascade.detectMultiScale(plate_img, scaleFactor = 1.3, minNeighbors = 7)\n",
    "\n",
    "    for (x,y,w,h) in plate_rect:\n",
    "        a,b = (int(0.02*img.shape[0]), int(0.025*img.shape[1])) #parameter tuning\n",
    "        plate = plate_img[y+a:y+h-a, x+b:x+w-b, :]\n",
    "        # finally representing the detected contours by drawing rectangles around the edges.\n",
    "        cv2.rectangle(plate_img, (x,y), (x+w, y+h), (51,51,255), 3)\n",
    "        \n",
    "    return plate_img, plate # returning the processed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2192af9",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'plate' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-162cde0d875b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'car.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplate_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_plate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-27a44963d14f>\u001b[0m in \u001b[0;36mextract_plate\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplate_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mplate_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplate\u001b[0m \u001b[0;31m# returning the processed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'plate' referenced before assignment"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('car.jpeg')\n",
    "plate = extract_plate(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0adf7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv2.imwrite('ex1.jpg',plate_img)\n",
    "cv2.imwrite('ex2.jpg', plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf355ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find characters in the resulting images\n",
    "def segment_characters(image) :\n",
    "\n",
    "    # Preprocess cropped license plate image\n",
    "    img = cv2.resize(image, (333, 75))\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary = cv2.threshold(img_gray, 200, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    img_erode = cv2.erode(img_binary, (3,3))\n",
    "    img_dilate = cv2.dilate(img_erode, (3,3))\n",
    "\n",
    "    LP_WIDTH = img_dilate.shape[0]\n",
    "    LP_HEIGHT = img_dilate.shape[1]\n",
    "\n",
    "    # Make borders white\n",
    "    img_dilate[0:3,:] = 255\n",
    "    img_dilate[:,0:3] = 255\n",
    "    img_dilate[72:75,:] = 255\n",
    "    img_dilate[:,330:333] = 255\n",
    "\n",
    "    # Estimations of character contours sizes of cropped license plates\n",
    "    dimensions = [LP_WIDTH/6, LP_WIDTH/2, LP_HEIGHT/10, 2*LP_HEIGHT/3]\n",
    "\n",
    "    # Get contours within cropped license plate\n",
    "    char_list = find_contours(dimensions, img_dilate)\n",
    "\n",
    "    return char_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f56cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "char = segment_characters(plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356116d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match contours to license plate or character template\n",
    "def find_contours(dimensions, img) :\n",
    "\n",
    "    # Find all contours in the image\n",
    "    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Retrieve potential dimensions\n",
    "    lower_width = dimensions[0]\n",
    "    upper_width = dimensions[1]\n",
    "    lower_height = dimensions[2]\n",
    "    upper_height = dimensions[3]\n",
    "    \n",
    "\n",
    "    # Check largest 5 or  15 contours for license plate or character respectively\n",
    "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
    "\n",
    "    x_cntr_list = []\n",
    "    target_contours = []\n",
    "    img_res = []\n",
    "    for cntr in cntrs :\n",
    "        #detects contour in binary image and returns the coordinates of rectangle enclosing it\n",
    "        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)\n",
    "        \n",
    "        #checking the dimensions of the contour to filter out the characters by contour's size\n",
    "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
    "            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours\n",
    "\n",
    "            char_copy = np.zeros((44,24))\n",
    "            #extracting each character using the enclosing rectangle's coordinates.\n",
    "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
    "            char = cv2.resize(char, (20, 40))\n",
    "\n",
    "            # Make result formatted for classification: invert colors\n",
    "            char = cv2.subtract(255, char)\n",
    "\n",
    "            # Resize the image to 24x44 with black border\n",
    "            char_copy[2:42, 2:22] = char\n",
    "            char_copy[0:2, :] = 0\n",
    "            char_copy[:, 0:2] = 0\n",
    "            char_copy[42:44, :] = 0\n",
    "            char_copy[:, 22:24] = 0\n",
    "\n",
    "            img_res.append(char_copy) #List that stores the character's binary image (unsorted)\n",
    "\n",
    "    #Return characters on ascending order with respect to the x-coordinate (most-left character first)\n",
    "    \n",
    "    #arbitrary function that stores sorted list of character indeces\n",
    "    indices = sorted(range(len(x_cntr_list)), key=lambda k: x_cntr_list[k])\n",
    "    img_res_copy = []\n",
    "    for idx in indices:\n",
    "        img_res_copy.append(img_res[idx])# stores character images according to their index\n",
    "    img_res = np.array(img_res_copy)\n",
    "\n",
    "    return img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "794f5149",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), input_shape=(28, 28, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=36, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55a14b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_1',\n",
       " 'layers': [{'class_name': 'Conv2D',\n",
       "   'config': {'name': 'conv2d_1',\n",
       "    'trainable': True,\n",
       "    'batch_input_shape': (None, 28, 28, 3),\n",
       "    'dtype': 'float32',\n",
       "    'filters': 32,\n",
       "    'kernel_size': (5, 5),\n",
       "    'strides': (1, 1),\n",
       "    'padding': 'valid',\n",
       "    'data_format': 'channels_last',\n",
       "    'dilation_rate': (1, 1),\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'MaxPooling2D',\n",
       "   'config': {'name': 'max_pooling2d_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'pool_size': (2, 2),\n",
       "    'padding': 'valid',\n",
       "    'strides': (2, 2),\n",
       "    'data_format': 'channels_last'}},\n",
       "  {'class_name': 'Dropout',\n",
       "   'config': {'name': 'dropout_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'rate': 0.4,\n",
       "    'noise_shape': None,\n",
       "    'seed': None}},\n",
       "  {'class_name': 'Flatten',\n",
       "   'config': {'name': 'flatten_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'data_format': 'channels_last'}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 128,\n",
       "    'activation': 'relu',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}},\n",
       "  {'class_name': 'Dense',\n",
       "   'config': {'name': 'dense_2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 36,\n",
       "    'activation': 'softmax',\n",
       "    'use_bias': True,\n",
       "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "     'config': {'scale': 1.0,\n",
       "      'mode': 'fan_avg',\n",
       "      'distribution': 'uniform',\n",
       "      'seed': None}},\n",
       "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "    'kernel_regularizer': None,\n",
       "    'bias_regularizer': None,\n",
       "    'activity_regularizer': None,\n",
       "    'kernel_constraint': None,\n",
       "    'bias_constraint': None}}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f2543dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 images belonging to 36 classes.\n",
      "Found 216 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, width_shift_range=0.05, height_shift_range=0.05)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',  # this is the target directory\n",
    "        target_size=(28,28),  # all images will be resized to 28x28\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        'data/val',  # this is the target directory\n",
    "        target_size=(28,28),  # all images will be resized to 28x28\n",
    "        batch_size=1,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55fb25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da121904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "864/864 [==============================] - 10s 11ms/step - loss: 1.4691 - accuracy: 0.6030 - val_loss: 0.0084 - val_accuracy: 0.8796\n",
      "Epoch 2/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.2225 - accuracy: 0.9352 - val_loss: 0.0586 - val_accuracy: 0.9074\n",
      "Epoch 3/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.2026 - accuracy: 0.9340 - val_loss: 0.0026 - val_accuracy: 0.9259\n",
      "Epoch 4/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.1219 - accuracy: 0.9641 - val_loss: 5.9605e-07 - val_accuracy: 0.9861\n",
      "Epoch 5/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0972 - accuracy: 0.9653 - val_loss: 0.0047 - val_accuracy: 0.9444\n",
      "Epoch 6/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.0842 - accuracy: 0.9722 - val_loss: 3.2186e-06 - val_accuracy: 0.9815\n",
      "Epoch 7/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0848 - accuracy: 0.9688 - val_loss: 0.0087 - val_accuracy: 0.9306\n",
      "Epoch 8/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0922 - accuracy: 0.9664 - val_loss: 5.7815e-05 - val_accuracy: 0.9815\n",
      "Epoch 9/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0625 - accuracy: 0.9734 - val_loss: 1.4305e-06 - val_accuracy: 0.9815\n",
      "Epoch 10/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.0753 - accuracy: 0.9722 - val_loss: 3.4732e-04 - val_accuracy: 0.9861\n",
      "Epoch 11/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.0463 - accuracy: 0.9780 - val_loss: 1.1921e-07 - val_accuracy: 0.9537\n",
      "Epoch 12/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.1103 - accuracy: 0.9630 - val_loss: 3.5763e-07 - val_accuracy: 0.9676\n",
      "Epoch 13/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0516 - accuracy: 0.9792 - val_loss: 0.7121 - val_accuracy: 0.9769\n",
      "Epoch 14/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0430 - accuracy: 0.9861 - val_loss: 0.0000e+00 - val_accuracy: 0.9907\n",
      "Epoch 15/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0539 - accuracy: 0.9734 - val_loss: 1.2560 - val_accuracy: 0.9444\n",
      "Epoch 16/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.0885 - accuracy: 0.9676 - val_loss: 2.9166e-04 - val_accuracy: 0.9676\n",
      "Epoch 17/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0508 - accuracy: 0.9826 - val_loss: 3.8861e-05 - val_accuracy: 0.9907\n",
      "Epoch 18/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.0514 - accuracy: 0.9769 - val_loss: 4.2080e-05 - val_accuracy: 1.0000\n",
      "Epoch 19/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0426 - accuracy: 0.9803 - val_loss: 0.0000e+00 - val_accuracy: 0.9954\n",
      "Epoch 20/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.0560 - accuracy: 0.9769 - val_loss: 5.8038e-04 - val_accuracy: 0.9907\n",
      "Epoch 21/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.0443 - accuracy: 0.9826 - val_loss: 0.0000e+00 - val_accuracy: 0.9630\n",
      "Epoch 22/23\n",
      "864/864 [==============================] - 8s 9ms/step - loss: 0.0467 - accuracy: 0.9803 - val_loss: 0.0000e+00 - val_accuracy: 0.9861\n",
      "Epoch 23/23\n",
      "864/864 [==============================] - 8s 10ms/step - loss: 0.0453 - accuracy: 0.9792 - val_loss: 3.5763e-07 - val_accuracy: 0.9722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa3680fae80>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "      steps_per_epoch = train_generator.samples // batch_size,\n",
    "      validation_data = validation_generator, \n",
    "      validation_steps = validation_generator.samples // batch_size,\n",
    "      epochs = 23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f367e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_plate(img): # the function detects and perfors blurring on the number plate.\n",
    "    plate_img = img.copy()\n",
    "    \n",
    "    #Loads the data required for detecting the license plates from cascade classifier.\n",
    "    plate_cascade = cv2.CascadeClassifier('./indian_license_plate.xml')\n",
    "\n",
    "    # detects numberplates and returns the coordinates and dimensions of detected license plate's contours.\n",
    "    plate_rect = plate_cascade.detectMultiScale(plate_img, scaleFactor = 1.3, minNeighbors = 7)\n",
    "\n",
    "    for (x,y,w,h) in plate_rect:\n",
    "        a,b = (int(0.02*img.shape[0]), int(0.025*img.shape[1])) #parameter tuning\n",
    "        plate = plate_img[y+a:y+h-a, x+b:x+w-b, :]\n",
    "        # finally representing the detected contours by drawing rectangles around the edges.\n",
    "        cv2.rectangle(plate_img, (x,y), (x+w, y+h), (51,51,255), 3)\n",
    "        \n",
    "    return plate_img, plate # returning the processed image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c037e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find characters in the resulting images\n",
    "def segment_characters(image) :\n",
    "\n",
    "    # Preprocess cropped license plate image\n",
    "    img = cv2.resize(image, (333, 75))\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary = cv2.threshold(img_gray, 200, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    img_erode = cv2.erode(img_binary, (3,3))\n",
    "    img_dilate = cv2.dilate(img_erode, (3,3))\n",
    "\n",
    "    LP_WIDTH = img_dilate.shape[0]\n",
    "    LP_HEIGHT = img_dilate.shape[1]\n",
    "\n",
    "    # Make borders white\n",
    "    img_dilate[0:3,:] = 255\n",
    "    img_dilate[:,0:3] = 255\n",
    "    img_dilate[72:75,:] = 255\n",
    "    img_dilate[:,330:333] = 255\n",
    "\n",
    "    # Estimations of character contours sizes of cropped license plates\n",
    "    dimensions = [LP_WIDTH/6, LP_WIDTH/2, LP_HEIGHT/10, 2*LP_HEIGHT/3]\n",
    "\n",
    "    # Get contours within cropped license plate\n",
    "    char_list = find_contours(dimensions, img_dilate)\n",
    "\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3462f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Match contours to license plate or character template\n",
    "def find_contours(dimensions, img) :\n",
    "\n",
    "    # Find all contours in the image\n",
    "    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Retrieve potential dimensions\n",
    "    lower_width = dimensions[0]\n",
    "    upper_width = dimensions[1]\n",
    "    lower_height = dimensions[2]\n",
    "    upper_height = dimensions[3]\n",
    "    \n",
    "\n",
    "    # Check largest 5 or  15 contours for license plate or character respectively\n",
    "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
    "\n",
    "    x_cntr_list = []\n",
    "    target_contours = []\n",
    "    img_res = []\n",
    "    for cntr in cntrs :\n",
    "        #detects contour in binary image and returns the coordinates of rectangle enclosing it\n",
    "        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)\n",
    "        \n",
    "        #checking the dimensions of the contour to filter out the characters by contour's size\n",
    "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
    "            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours\n",
    "\n",
    "            char_copy = np.zeros((44,24))\n",
    "            #extracting each character using the enclosing rectangle's coordinates.\n",
    "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
    "            char = cv2.resize(char, (20, 40))\n",
    "\n",
    "            # Make result formatted for classification: invert colors\n",
    "            char = cv2.subtract(255, char)\n",
    "\n",
    "            # Resize the image to 24x44 with black border\n",
    "            char_copy[2:42, 2:22] = char\n",
    "            char_copy[0:2, :] = 0\n",
    "            char_copy[:, 0:2] = 0\n",
    "            char_copy[42:44, :] = 0\n",
    "            char_copy[:, 22:24] = 0\n",
    "\n",
    "            img_res.append(char_copy) #List that stores the character's binary image (unsorted)\n",
    "\n",
    "    #Return characters on ascending order with respect to the x-coordinate (most-left character first)\n",
    "    \n",
    "    #arbitrary function that stores sorted list of character indeces\n",
    "    indices = sorted(range(len(x_cntr_list)), key=lambda k: x_cntr_list[k])\n",
    "    img_res_copy = []\n",
    "    for idx in indices:\n",
    "        img_res_copy.append(img_res[idx])# stores character images according to their index\n",
    "    img_res = np.array(img_res_copy)\n",
    "\n",
    "    return img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7b337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac225d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1cf21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09036763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "313d60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dimension(img):\n",
    "    new_img = np.zeros((28,28,3))\n",
    "    for i in range(3):\n",
    "        new_img[:,:,i] = img\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fb66b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results():\n",
    "    dic = {}\n",
    "    characters = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    for i,c in enumerate(characters):\n",
    "        dic[i] = c\n",
    "    \n",
    "    output = []\n",
    "    for i, ch in enumerate(char):\n",
    "        img_ = cv2.resize(ch, (28,28),interpolation=cv2.INTER_AREA)\n",
    "        img = fix_dimension(img_)\n",
    "        img = img.reshape(1,28,28,3)\n",
    "        y_ = model.predict_classes(img)[0]\n",
    "        character = dic[y_]\n",
    "        output.append(character)\n",
    "        \n",
    "    plate_number = ''.join(output)\n",
    "    \n",
    "    return plate_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36d89d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16DHQ779'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img = cv2.imread('car2.jpeg')\n",
    "show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5843912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Car-Detective.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415552e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
